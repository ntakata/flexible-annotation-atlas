{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code prepares AObase.json and AVbase.nrrd using online resources at AIBS (annotation_100.nrrd and 1.json; Put these in the data folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "import sys\n",
    "import os\n",
    "from nbparameterise import(extract_parameters, replace_definitions, parameter_values)\n",
    "\n",
    "path_datafolder = os.path.join(os.getcwd(), 'data')\n",
    "dir_notebooks = 'notebooks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare anatomical ontology(AO)_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add_VC_to_AO\n",
    "VC: voxel count, AO: annotation ontoloy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cells': [{'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': \"# Add voxel-counts in annotation volume to an anatomical ontology file\\nUsing the original annotation volume (AV) and anatomical ontology (AO) file of the mouse brain by Allen Institute for Brain Science (AIBS), this notebook checks voxel-counts (VC) for each brain structure in AV, and adds them to AO file.\\n\\nPrepare directories 'data', 'gene_data', 'fiber_data' and 'figs' at a working directory. Put the original files by AIBS, '1.json' and 'annotation_100.nrrd', at 'data' folder. Output files are saved in the above folders.\\n\\n- input files\\n    - annotation_100.nrrd: The original AV by AIBS\\n    - 1.json: The original AO file by AIBS\\n- output file\\n    - 1_VC.json: AO file with voxel counts for each brain structure\"},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Set variables'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 1,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': \"dir_data = 'D:\\\\\\\\ES\\\\\\\\Projects\\\\\\\\FlexibleAtlas\\\\\\\\github\\\\\\\\data'\\n_case = 'VC_to_AO_original'\\nfn_input_AV = 'annotation_100.nrrd'\\nfn_input_AO = '1.json'\\nfn_output_AO = '1_VC.json'\\nfn_input_AV = 'annotation_100.nrrd'\\nfn_input_AO = '1.json'\\nfn_output_AO = '1_VC.json'\\nfn_input_AV = 'annotation_100.nrrd'\\nfn_input_AO = '1.json'\\nfn_output_AO = '1_VC.json'\\nfn_input_AV = 'annotation_100.nrrd'\\nfn_input_AO = '1.json'\\nfn_output_AO = '1_VC.json'\\nfn_input_AO = '1.json'\\nfn_input_AV = 'annotation_100.nrrd'\\nfn_output_AO = '1_VC.json'\\nfn_input_AO = '1.json'\\nfn_input_AV = 'annotation_100.nrrd'\\nfn_output_AO = '1_VC.json'\"},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 2,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'import os\\nimport nrrd\\nimport numpy as np\\nimport pandas as pd\\nimport json\\nimport copy\\nfrom collections import OrderedDict\\nfrom jsonpath_rw import jsonpath, parse'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Load data'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 3,\n",
       "    'metadata': {'code_folding': []},\n",
       "    'outputs': [],\n",
       "    'source': \"AV, Header = nrrd.read(os.path.join(dir_data, fn_input_AV))\\n\\nID_unique = np.unique(AV)\\nIDvc_list = [[i, np.sum(AV == i)] for i in ID_unique]\\nIDvc = pd.DataFrame(IDvc_list, columns=['ID', 'voxel_count'])\\n\\nwith open(os.path.join(dir_data, fn_input_AO)) as f:\\n    df = json.load(f, object_pairs_hook=OrderedDict)\"},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Add voxel-counts to an annotation ontology file'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 4,\n",
       "    'metadata': {'code_folding': []},\n",
       "    'outputs': [],\n",
       "    'source': 'def Add_VC_to_AO(match_id, match_fullpath):\\n    global df_VC\\n    global IDvc\\n    if IDvc[\\'ID\\'].isin([match_id]).any():\\n        temp_voxelcount = IDvc.loc[IDvc[\\'ID\\'] == match_id, \\'voxel_count\\'].tolist()[0]\\n    else:\\n        temp_voxelcount = None\\n    exec(\"df_VC[\\'msg\\'][0]\"+\\\\\\n         str(match_fullpath).replace(\\'.\\',\\'\\')\\\\\\n          .replace(\\'children\\', \"[\\'children\\']\").replace(\\'id\\', \\'\\')+\\\\\\n         \"[\\'voxel_count\\'] = \"+ str(temp_voxelcount))\\n    return'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 5,\n",
       "    'metadata': {'code_folding': []},\n",
       "    'outputs': [],\n",
       "    'source': \"jsonpath_expr = parse('$..id')\\ndf_VC = copy.deepcopy(df)\\n[Add_VC_to_AO(match.value, match.full_path) \\\\\\n for match in jsonpath_expr.find(df_VC['msg'][0])];\"},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Save AO with voxel counts'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 6,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': \"with open(os.path.join(dir_data, fn_output_AO), mode='w') as fw:\\n    json.dump(df_VC, fw, indent=4)\"},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Check data'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 7,\n",
       "    'metadata': {},\n",
       "    'outputs': [{'output_type': 'stream',\n",
       "      'name': 'stdout',\n",
       "      'text': 'Data type of the annotation volume:  uint32\\n# unique ID:  670\\n# total voxel:  1203840\\n# brain voxel:  505359\\n# outside brain voxel:  698481\\n'},\n",
       "     {'output_type': 'execute_result',\n",
       "      'metadata': {},\n",
       "      'data': {'text/plain': '   ID  voxel_count\\n0   0       698481\\n1   1          107\\n2   2          119\\n3   6         2058\\n4   7         1108',\n",
       "       'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>ID</th>\\n      <th>voxel_count</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>0</td>\\n      <td>698481</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>1</td>\\n      <td>107</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2</td>\\n      <td>119</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>6</td>\\n      <td>2058</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>7</td>\\n      <td>1108</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'},\n",
       "      'execution_count': 7}],\n",
       "    'source': 'print(\"Data type of the annotation volume: \", AV.dtype) # uint32\\nprint(\"# unique ID: \", np.unique(AV).size) # There is 670 unique ID in AV\\nprint(\\'# total voxel: \\', np.size(AV)) # 1203840 voxels\\nprint(\\'# brain voxel: \\', np.sum(AV!=0)) # 505359\\nprint(\\'# outside brain voxel: \\', np.sum(AV==0)) # 698481\\nIDvc.head()'}],\n",
       "  'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "    'language': 'python',\n",
       "    'name': 'python3'},\n",
       "   'language_info': {'name': 'python',\n",
       "    'version': '3.7.1',\n",
       "    'mimetype': 'text/x-python',\n",
       "    'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "    'pygments_lexer': 'ipython3',\n",
       "    'nbconvert_exporter': 'python',\n",
       "    'file_extension': '.py'},\n",
       "   'toc': {'base_numbering': 1,\n",
       "    'nav_menu': {},\n",
       "    'number_sections': True,\n",
       "    'sideBar': True,\n",
       "    'skip_h1_title': False,\n",
       "    'title_cell': 'Table of Contents',\n",
       "    'title_sidebar': 'Contents',\n",
       "    'toc_cell': False,\n",
       "    'toc_position': {'height': 'calc(100% - 180px)',\n",
       "     'left': '10px',\n",
       "     'top': '150px',\n",
       "     'width': '384px'},\n",
       "    'toc_section_display': True,\n",
       "    'toc_window_display': True}},\n",
       "  'nbformat': 4,\n",
       "  'nbformat_minor': 2},\n",
       " {'metadata': {'path': './notebooks'}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(dir_notebooks,'Add_VC_to_AO.ipynb')) as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "    \n",
    "orig_parameters = extract_parameters(nb)\n",
    "params = parameter_values(orig_parameters,\\\n",
    "                          dir_data=path_datafolder,\\\n",
    "                          fn_input_AV='annotation_100.nrrd',\\\n",
    "                          fn_input_AO='1.json',\\\n",
    "                          fn_output_AO='1_VC.json')\n",
    "new_nb = replace_definitions(nb, params)\n",
    "\n",
    "ep = ExecutePreprocessor(timeout=600, kernel_name='python3')\n",
    "ep.preprocess(new_nb, {'metadata':{'path':'./notebooks'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune_leaf_ROI_wo_VC_in_AO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cells': [{'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Prune leaf ROIs in an annotation ontology file without voxel-counts\\nThis notebook deletes nodes in an anatomical ontology (AO) text-file if these are leaf nodes without voxel-counts (VC) in annotation volume (AV).\\n\\n- input\\n    - 1_VC.json: AO file with VC\\n- output\\n    - 1_VC_pruned.json: pruned AO file with VC'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Set variables'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 1,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': \"dir_data = 'D:\\\\\\\\ES\\\\\\\\Projects\\\\\\\\FlexibleAtlas\\\\\\\\github\\\\\\\\data'\\nfn_input_AO = '1_VC.json'\\nfn_output_AO = '1_VC_pruned.json'\"},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 2,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'import os\\nimport json\\nimport copy\\nimport numpy as np\\nfrom collections import OrderedDict\\nfrom jsonpath_rw import jsonpath, parse'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Load data'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 3,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'with open(os.path.join(dir_data, fn_input_AO)) as f:\\n    df_VC = json.load(f, object_pairs_hook=OrderedDict)'},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': \"# Delete ROIs in an annotation ontology file if it's a leaf without voxel counts\"},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 4,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'def Delete_leaf_node_WO_VC(match_id, match_fullpath):\\n    global df_VC_pruned\\n    global leaf_count\\n    bool1 = eval((\"df_VC_pruned[\\'msg\\'][0]\"+\\\\\\n                  str(match_fullpath).replace(\\'.\\',\\'\\')\\\\\\n                  .replace(\\'children\\', \"[\\'children\\']\")\\\\\\n                  .replace(\\'id\\',\\'\\')+\"[\\'children\\'] == []\"))\\n    bool2 = eval((\"df_VC_pruned[\\'msg\\'][0]\"+\\\\\\n                  str(match_fullpath).replace(\\'.\\',\\'\\')\\\\\\n                  .replace(\\'children\\', \"[\\'children\\']\")\\\\\\n                  .replace(\\'id\\',\\'\\')+\"[\\'voxel_count\\'] is None\"))\\n    if bool1:\\n        leaf_count = leaf_count + 1\\n    if bool1 and bool2:\\n        print((\"***** deleted ID: \" + \\\\\\n              str(eval(\"df_VC_pruned[\\'msg\\'][0]\"+\\\\\\n                       str(match_fullpath).replace(\\'.\\',\\'\\')\\\\\\n                       .replace(\\'children\\', \"[\\'children\\']\")\\\\\\n                       .replace(\\'id\\', \"[\\'id\\']\")))))\\n        exec(\"del df_VC_pruned[\\'msg\\'][0]\"+\\\\\\n             str(match_fullpath).replace(\\'.\\',\\'\\')\\\\\\n             .replace(\\'children\\', \"[\\'children\\']\")\\\\\\n             .replace(\\'id\\', \\'\\'))\\n        return match_id * (-1) # deleted ids are assigned as negative\\n    else:\\n        print((\"remained ID: \" + \\n               str(eval(\"df_VC_pruned[\\'msg\\'][0]\"+\\\\\\n                        str(match_fullpath)\\\\\\n                        .replace(\\'.\\',\\'\\')\\\\\\n                        .replace(\\'children\\', \"[\\'children\\']\")\\\\\\n                        .replace(\\'id\\', \"[\\'id\\']\")))))\\n        return match_id # remained ids in positive'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 5,\n",
       "    'metadata': {},\n",
       "    'outputs': [{'output_type': 'stream',\n",
       "      'name': 'stdout',\n",
       "      'text': '***** deleted ID: 304325711\\n***** deleted ID: 624\\n***** deleted ID: 65\\n***** deleted ID: 57\\n***** deleted ID: 49\\n***** deleted ID: 43\\n***** deleted ID: 34\\n***** deleted ID: 25\\n***** deleted ID: 18\\n***** deleted ID: 11\\n***** deleted ID: 3\\n***** deleted ID: 1119\\n***** deleted ID: 1112\\n***** deleted ID: 1103\\n***** deleted ID: 1095\\n***** deleted ID: 1087\\n***** deleted ID: 1040\\n***** deleted ID: 1078\\n***** deleted ID: 1071\\n***** deleted ID: 1063\\n***** deleted ID: 1055\\n***** deleted ID: 1032\\n***** deleted ID: 1024\\n***** deleted ID: 164\\nremained ID: 153\\nremained ID: 145\\nremained ID: 140\\nremained ID: 129\\n***** deleted ID: 124\\n***** deleted ID: 116\\nremained ID: 108\\nremained ID: 98\\n***** deleted ID: 89\\nremained ID: 81\\nremained ID: 73\\n***** deleted ID: 563\\n***** deleted ID: 547\\n***** deleted ID: 70\\n***** deleted ID: 730\\nremained ID: 611\\nremained ID: 595\\nremained ID: 802\\nremained ID: 1083\\n***** deleted ID: 722\\n***** deleted ID: 1068\\nremained ID: 673\\nremained ID: 681\\nremained ID: 690\\nremained ID: 753\\nremained ID: 46\\n***** deleted ID: 150\\n***** deleted ID: 787\\n***** deleted ID: 779\\n***** deleted ID: 770\\n***** deleted ID: 762\\n***** deleted ID: 182\\n***** deleted ID: 341\\n***** deleted ID: 166\\n***** deleted ID: 833\\n***** deleted ID: 825\\n***** deleted ID: 817\\nremained ID: 349\\n***** deleted ID: 174\\n***** deleted ID: 405\\nremained ID: 54\\nremained ID: 824\\nremained ID: 484682528\\nremained ID: 301\\n***** deleted ID: 37\\n***** deleted ID: 474\\n***** deleted ID: 713\\nremained ID: 449\\nremained ID: 443\\nremained ID: 618\\nremained ID: 436\\nremained ID: 428\\nremained ID: 737\\n***** deleted ID: 420\\n***** deleted ID: 745\\nremained ID: 603\\nremained ID: 530\\nremained ID: 466\\nremained ID: 1099\\nremained ID: 940\\nremained ID: 908\\n***** deleted ID: 892\\nremained ID: 884\\nremained ID: 768\\nremained ID: 991\\n***** deleted ID: 941\\n***** deleted ID: 213\\n***** deleted ID: 205\\n***** deleted ID: 855\\n***** deleted ID: 736\\n***** deleted ID: 221\\nremained ID: 397\\nremained ID: 863\\nremained ID: 1043\\nremained ID: 1060\\n***** deleted ID: 1051\\nremained ID: 877\\n***** deleted ID: 317\\n***** deleted ID: 309\\n***** deleted ID: 134\\n***** deleted ID: 109\\nremained ID: 102\\n***** deleted ID: 142\\nremained ID: 760\\nremained ID: 1000\\nremained ID: 484682524\\nremained ID: 484682520\\n***** deleted ID: 365\\n***** deleted ID: 86\\n***** deleted ID: 14\\nremained ID: 1092\\nremained ID: 896\\n***** deleted ID: 1028\\n***** deleted ID: 1019\\nremained ID: 198\\nremained ID: 190\\n***** deleted ID: 994\\n***** deleted ID: 1003\\n***** deleted ID: 1012\\n***** deleted ID: 1036\\nremained ID: 924\\nremained ID: 6\\nremained ID: 784\\nremained ID: 986\\nremained ID: 484682516\\n***** deleted ID: 979\\nremained ID: 971\\nremained ID: 1108\\nremained ID: 964\\nremained ID: 579\\nremained ID: 956\\nremained ID: 776\\nremained ID: 983\\nremained ID: 484682512\\nremained ID: 728\\n***** deleted ID: 373\\n***** deleted ID: 410\\n***** deleted ID: 404\\n***** deleted ID: 490\\n***** deleted ID: 650\\n***** deleted ID: 499\\nremained ID: 553\\nremained ID: 1123\\nremained ID: 78\\nremained ID: 866\\nremained ID: 850\\n***** deleted ID: 85\\nremained ID: 812\\nremained ID: 326\\nremained ID: 752\\nremained ID: 744\\nremained ID: 960\\n***** deleted ID: 627\\n***** deleted ID: 285\\n***** deleted ID: 253\\n***** deleted ID: 277\\n***** deleted ID: 293\\n***** deleted ID: 270\\n***** deleted ID: 261\\n***** deleted ID: 245\\n***** deleted ID: 389\\n***** deleted ID: 29\\n***** deleted ID: 871\\nremained ID: 697\\n***** deleted ID: 396\\n***** deleted ID: 388\\nremained ID: 380\\nremained ID: 514\\n***** deleted ID: 586\\n***** deleted ID: 858\\n***** deleted ID: 522\\n***** deleted ID: 570\\nremained ID: 932\\nremained ID: 792\\n***** deleted ID: 925\\n***** deleted ID: 813\\n***** deleted ID: 717\\nremained ID: 237\\nremained ID: 917\\n***** deleted ID: 808\\nremained ID: 482\\nremained ID: 633\\nremained ID: 658\\nremained ID: 506\\n***** deleted ID: 641\\nremained ID: 841\\nremained ID: 948\\nremained ID: 413\\n***** deleted ID: 1076\\nremained ID: 933\\nremained ID: 1116\\n***** deleted ID: 1131\\nremained ID: 798\\nremained ID: 794\\n***** deleted ID: 705\\nremained ID: 229\\nremained ID: 93\\nremained ID: 901\\n***** deleted ID: 710\\n***** deleted ID: 384\\nremained ID: 911\\nremained ID: 158\\nremained ID: 62\\nremained ID: 832\\n***** deleted ID: 357\\nremained ID: 125\\nremained ID: 117\\nremained ID: 336\\nremained ID: 916\\n***** deleted ID: 876\\nremained ID: 848\\nremained ID: 900\\n***** deleted ID: 459\\nremained ID: 538\\nremained ID: 665\\nremained ID: 21\\nremained ID: 1016\\nremained ID: 840\\nremained ID: 949\\n***** deleted ID: 885\\nremained ID: 967\\nremained ID: 1009\\nremained ID: 589508455\\nremained ID: 846\\nremained ID: 91\\nremained ID: 989\\nremained ID: 519\\n***** deleted ID: 10690\\n***** deleted ID: 10691\\n***** deleted ID: 10692\\nremained ID: 1049\\n***** deleted ID: 10687\\n***** deleted ID: 10688\\n***** deleted ID: 10689\\nremained ID: 1041\\n***** deleted ID: 10684\\n***** deleted ID: 10685\\n***** deleted ID: 10686\\nremained ID: 1033\\n***** deleted ID: 10681\\n***** deleted ID: 10682\\n***** deleted ID: 10683\\nremained ID: 1025\\n***** deleted ID: 10678\\n***** deleted ID: 10679\\n***** deleted ID: 10680\\nremained ID: 1064\\n***** deleted ID: 10675\\n***** deleted ID: 10676\\n***** deleted ID: 10677\\nremained ID: 1056\\nremained ID: 1017\\n***** deleted ID: 10672\\n***** deleted ID: 10673\\n***** deleted ID: 10674\\nremained ID: 1007\\nremained ID: 1073\\n***** deleted ID: 10735\\n***** deleted ID: 10736\\n***** deleted ID: 10737\\nremained ID: 968\\n***** deleted ID: 10732\\n***** deleted ID: 10733\\n***** deleted ID: 10734\\nremained ID: 957\\n***** deleted ID: 10729\\n***** deleted ID: 10730\\n***** deleted ID: 10731\\nremained ID: 951\\n***** deleted ID: 10726\\n***** deleted ID: 10727\\n***** deleted ID: 10728\\nremained ID: 944\\n***** deleted ID: 10723\\n***** deleted ID: 10724\\n***** deleted ID: 10725\\nremained ID: 936\\n***** deleted ID: 10720\\n***** deleted ID: 10721\\n***** deleted ID: 10722\\nremained ID: 1091\\n***** deleted ID: 10717\\n***** deleted ID: 10718\\n***** deleted ID: 10719\\n***** deleted ID: 1001\\n***** deleted ID: 10714\\n***** deleted ID: 10715\\n***** deleted ID: 10716\\n***** deleted ID: 992\\nremained ID: 928\\n***** deleted ID: 10711\\n***** deleted ID: 10712\\n***** deleted ID: 10713\\nremained ID: 984\\n***** deleted ID: 10708\\n***** deleted ID: 10709\\n***** deleted ID: 10710\\nremained ID: 976\\nremained ID: 920\\n***** deleted ID: 10705\\n***** deleted ID: 10706\\n***** deleted ID: 10707\\nremained ID: 912\\nremained ID: 645\\n***** deleted ID: 1143\\n***** deleted ID: 1145\\n***** deleted ID: 1144\\nremained ID: 528\\nremained ID: 512\\nremained ID: 222\\nremained ID: 230\\nremained ID: 206\\nremained ID: 379\\n***** deleted ID: 76\\nremained ID: 781\\nremained ID: 773\\nremained ID: 765\\nremained ID: 217\\nremained ID: 225\\nremained ID: 202\\nremained ID: 209\\nremained ID: 701\\n***** deleted ID: 193\\n***** deleted ID: 185\\nremained ID: 1069\\n***** deleted ID: 995\\nremained ID: 169\\nremained ID: 177\\n***** deleted ID: 161\\nremained ID: 154\\nremained ID: 978\\nremained ID: 970\\nremained ID: 938\\nremained ID: 859\\nremained ID: 852\\nremained ID: 1107\\nremained ID: 1098\\nremained ID: 395\\nremained ID: 307\\nremained ID: 963\\nremained ID: 955\\nremained ID: 235\\nremained ID: 203\\nremained ID: 106\\nremained ID: 136\\nremained ID: 83\\nremained ID: 372\\nremained ID: 1048\\n***** deleted ID: 887\\nremained ID: 839\\nremained ID: 143\\nremained ID: 939\\nremained ID: 135\\n***** deleted ID: 640\\nremained ID: 576\\nremained ID: 661\\n***** deleted ID: 568\\nremained ID: 653\\nremained ID: 370\\n***** deleted ID: 789\\nremained ID: 589508451\\n***** deleted ID: 69\\n***** deleted ID: 45\\n***** deleted ID: 61\\n***** deleted ID: 53\\n***** deleted ID: 77\\nremained ID: 445\\nremained ID: 437\\nremained ID: 429\\n***** deleted ID: 691\\n***** deleted ID: 682\\n***** deleted ID: 674\\n***** deleted ID: 666\\n***** deleted ID: 659\\nremained ID: 651\\nremained ID: 642\\nremained ID: 903\\nremained ID: 1039\\nremained ID: 711\\nremained ID: 720\\nremained ID: 101\\nremained ID: 96\\n***** deleted ID: 560\\n***** deleted ID: 112\\nremained ID: 607\\nremained ID: 207\\nremained ID: 386\\nremained ID: 354\\nremained ID: 358\\nremained ID: 350\\nremained ID: 238\\nremained ID: 146\\nremained ID: 604\\nremained ID: 162\\nremained ID: 147\\n***** deleted ID: 130\\n***** deleted ID: 137\\nremained ID: 679\\nremained ID: 1117\\nremained ID: 549009227\\nremained ID: 549009223\\nremained ID: 549009219\\nremained ID: 549009215\\nremained ID: 621\\nremained ID: 574\\nremained ID: 534\\n***** deleted ID: 462\\nremained ID: 318\\n***** deleted ID: 552\\nremained ID: 1093\\nremained ID: 931\\nremained ID: 898\\nremained ID: 599626927\\n***** deleted ID: 283\\nremained ID: 880\\nremained ID: 280\\nremained ID: 987\\nremained ID: 114\\nremained ID: 105\\nremained ID: 122\\nremained ID: 398\\n***** deleted ID: 923\\n***** deleted ID: 915\\n***** deleted ID: 899\\n***** deleted ID: 890\\n***** deleted ID: 891\\n***** deleted ID: 883\\n***** deleted ID: 875\\n***** deleted ID: 868\\n***** deleted ID: 860\\n***** deleted ID: 881\\nremained ID: 123\\nremained ID: 867\\nremained ID: 7\\n***** deleted ID: 99\\n***** deleted ID: 90\\n***** deleted ID: 82\\nremained ID: 612\\nremained ID: 1132\\nremained ID: 771\\nremained ID: 1065\\nremained ID: 872\\nremained ID: 591\\nremained ID: 197\\nremained ID: 607344862\\nremained ID: 607344858\\nremained ID: 607344854\\nremained ID: 607344850\\nremained ID: 607344846\\nremained ID: 607344842\\nremained ID: 607344838\\nremained ID: 607344834\\nremained ID: 100\\nremained ID: 12\\nremained ID: 165\\nremained ID: 1052\\nremained ID: 374\\nremained ID: 348\\n***** deleted ID: 615\\nremained ID: 58\\nremained ID: 75\\nremained ID: 66\\nremained ID: 231\\nremained ID: 757\\nremained ID: 606826663\\nremained ID: 115\\nremained ID: 975\\nremained ID: 549009211\\nremained ID: 35\\nremained ID: 214\\nremained ID: 616\\n***** deleted ID: 549009207\\nremained ID: 549009203\\nremained ID: 1061\\nremained ID: 706\\nremained ID: 634\\nremained ID: 628\\nremained ID: 531\\nremained ID: 215\\nremained ID: 1100\\nremained ID: 614454277\\nremained ID: 587\\nremained ID: 67\\nremained ID: 50\\nremained ID: 795\\n***** deleted ID: 511\\n***** deleted ID: 503\\n***** deleted ID: 494\\nremained ID: 10\\nremained ID: 17\\nremained ID: 42\\nremained ID: 26\\nremained ID: 294\\n***** deleted ID: 555\\n***** deleted ID: 548\\n***** deleted ID: 539\\nremained ID: 128\\nremained ID: 246\\nremained ID: 607344830\\nremained ID: 749\\nremained ID: 381\\nremained ID: 323\\nremained ID: 599626923\\nremained ID: 460\\nremained ID: 874\\nremained ID: 271\\nremained ID: 580\\nremained ID: 828\\nremained ID: 820\\nremained ID: 811\\nremained ID: 4\\nremained ID: 834\\nremained ID: 842\\nremained ID: 851\\nremained ID: 302\\nremained ID: 339\\nremained ID: 313\\nremained ID: 10671\\nremained ID: 804\\n***** deleted ID: 796\\nremained ID: 797\\nremained ID: 614\\nremained ID: 470\\nremained ID: 173\\nremained ID: 576073704\\nremained ID: 364\\nremained ID: 356\\nremained ID: 226\\nremained ID: 194\\nremained ID: 290\\nremained ID: 946\\n***** deleted ID: 785\\n***** deleted ID: 777\\n***** deleted ID: 769\\n***** deleted ID: 761\\nremained ID: 693\\n***** deleted ID: 464\\n***** deleted ID: 455\\n***** deleted ID: 447\\n***** deleted ID: 439\\nremained ID: 63\\nremained ID: 1004\\nremained ID: 980\\n***** deleted ID: 756\\n***** deleted ID: 748\\n***** deleted ID: 740\\nremained ID: 515\\nremained ID: 1\\nremained ID: 1126\\nremained ID: 557\\n***** deleted ID: 1118\\n***** deleted ID: 1110\\nremained ID: 525\\nremained ID: 606826659\\nremained ID: 606826655\\nremained ID: 606826651\\nremained ID: 606826647\\nremained ID: 732\\nremained ID: 491\\nremained ID: 210\\nremained ID: 331\\n***** deleted ID: 724\\n***** deleted ID: 716\\n***** deleted ID: 708\\n***** deleted ID: 700\\nremained ID: 88\\nremained ID: 467\\nremained ID: 689\\nremained ID: 576073699\\nremained ID: 338\\nremained ID: 286\\nremained ID: 347\\nremained ID: 133\\nremained ID: 126\\n***** deleted ID: 1124\\nremained ID: 1109\\nremained ID: 914\\nremained ID: 763\\nremained ID: 523\\nremained ID: 452\\n***** deleted ID: 684\\n***** deleted ID: 676\\n***** deleted ID: 668\\nremained ID: 830\\nremained ID: 272\\nremained ID: 263\\n***** deleted ID: 80\\nremained ID: 72\\nremained ID: 141\\nremained ID: 223\\nremained ID: 118\\nremained ID: 30\\n***** deleted ID: 110\\n***** deleted ID: 87\\n***** deleted ID: 55\\n***** deleted ID: 94\\n***** deleted ID: 660\\n***** deleted ID: 652\\n***** deleted ID: 103\\n***** deleted ID: 79\\n***** deleted ID: 47\\n***** deleted ID: 71\\nremained ID: 38\\n***** deleted ID: 432\\nremained ID: 332\\nremained ID: 390\\nremained ID: 157\\nremained ID: 1097\\n***** deleted ID: 953\\nremained ID: 186\\nremained ID: 483\\nremained ID: 958\\nremained ID: 321\\n***** deleted ID: 316\\n***** deleted ID: 300\\nremained ID: 178\\nremained ID: 563807439\\nremained ID: 27\\nremained ID: 1014\\nremained ID: 262\\nremained ID: 560581563\\nremained ID: 930\\nremained ID: 575\\nremained ID: 907\\nremained ID: 599\\nremained ID: 189\\nremained ID: 51\\nremained ID: 560581559\\nremained ID: 181\\nremained ID: 15\\nremained ID: 149\\nremained ID: 571\\nremained ID: 1077\\nremained ID: 366\\n***** deleted ID: 636\\n***** deleted ID: 626\\n***** deleted ID: 617\\nremained ID: 362\\nremained ID: 59\\nremained ID: 444\\nremained ID: 155\\nremained ID: 1113\\nremained ID: 1120\\nremained ID: 64\\nremained ID: 1104\\nremained ID: 1096\\nremained ID: 127\\nremained ID: 255\\nremained ID: 239\\n***** deleted ID: 560581555\\nremained ID: 560581551\\nremained ID: 325\\nremained ID: 1029\\nremained ID: 1020\\nremained ID: 218\\nremained ID: 138\\nremained ID: 856\\nremained ID: 496345672\\nremained ID: 496345668\\nremained ID: 496345664\\nremained ID: 170\\nremained ID: 1088\\nremained ID: 1079\\nremained ID: 1072\\nremained ID: 475\\nremained ID: 1008\\nremained ID: 1044\\nremained ID: 609\\nremained ID: 422\\nremained ID: 414\\nremained ID: 406\\nremained ID: 563807435\\nremained ID: 741\\nremained ID: 733\\nremained ID: 725\\nremained ID: 718\\nremained ID: 709\\nremained ID: 685\\nremained ID: 629\\nremained ID: 637\\nremained ID: 864\\nremained ID: 549\\nremained ID: 1129\\nremained ID: 343\\nremained ID: 287\\n***** deleted ID: 602\\n***** deleted ID: 594\\n***** deleted ID: 585\\n***** deleted ID: 578\\n***** deleted ID: 569\\n***** deleted ID: 367\\n***** deleted ID: 529\\n***** deleted ID: 562\\n***** deleted ID: 554\\n***** deleted ID: 521\\n***** deleted ID: 546\\n***** deleted ID: 513\\n***** deleted ID: 505\\n***** deleted ID: 498\\n***** deleted ID: 537\\n***** deleted ID: 359\\nremained ID: 351\\nremained ID: 809\\nremained ID: 581\\nremained ID: 596\\nremained ID: 564\\nremained ID: 904\\nremained ID: 826\\nremained ID: 298\\nremained ID: 342\\nremained ID: 835\\nremained ID: 1031\\nremained ID: 1022\\nremained ID: 818\\nremained ID: 803\\n***** deleted ID: 435\\n***** deleted ID: 487\\n***** deleted ID: 480\\n***** deleted ID: 472\\n***** deleted ID: 426\\n***** deleted ID: 418\\n***** deleted ID: 411\\nremained ID: 403\\nremained ID: 1105\\nremained ID: 559\\nremained ID: 551\\nremained ID: 544\\nremained ID: 536\\nremained ID: 292\\nremained ID: 23\\nremained ID: 278\\nremained ID: 333\\nremained ID: 310\\nremained ID: 266\\nremained ID: 258\\nremained ID: 250\\nremained ID: 242\\nremained ID: 275\\n***** deleted ID: 549009199\\n***** deleted ID: 473\\n***** deleted ID: 465\\n***** deleted ID: 458\\n***** deleted ID: 144\\n***** deleted ID: 489\\n***** deleted ID: 481\\nremained ID: 754\\nremained ID: 998\\nremained ID: 56\\nremained ID: 493\\nremained ID: 672\\nremained ID: 485\\nremained ID: 477\\nremained ID: 623\\nremained ID: 780\\nremained ID: 334\\nremained ID: 327\\nremained ID: 319\\nremained ID: 451\\nremained ID: 311\\nremained ID: 303\\nremained ID: 295\\nremained ID: 131\\nremained ID: 966\\nremained ID: 952\\nremained ID: 942\\nremained ID: 583\\n***** deleted ID: 16\\nremained ID: 703\\nremained ID: 484682508\\nremained ID: 589508447\\n***** deleted ID: 484682504\\n***** deleted ID: 484682500\\n***** deleted ID: 484682496\\n***** deleted ID: 484682492\\n***** deleted ID: 484682487\\n***** deleted ID: 484682483\\n***** deleted ID: 484682479\\n***** deleted ID: 484682475\\nremained ID: 484682470\\n***** deleted ID: 861\\n***** deleted ID: 870\\n***** deleted ID: 853\\n***** deleted ID: 518\\n***** deleted ID: 837\\n***** deleted ID: 845\\n***** deleted ID: 829\\n***** deleted ID: 509\\nremained ID: 502\\n***** deleted ID: 10701\\n***** deleted ID: 10700\\n***** deleted ID: 10699\\nremained ID: 1084\\n***** deleted ID: 10698\\n***** deleted ID: 10697\\n***** deleted ID: 10696\\nremained ID: 1037\\n***** deleted ID: 10695\\n***** deleted ID: 10694\\n***** deleted ID: 10693\\nremained ID: 843\\n***** deleted ID: 1133\\n***** deleted ID: 419\\n***** deleted ID: 371\\n***** deleted ID: 324\\n***** deleted ID: 259\\n***** deleted ID: 934\\nremained ID: 743\\n***** deleted ID: 550\\nremained ID: 727\\n***** deleted ID: 712\\nremained ID: 664\\n***** deleted ID: 508\\n***** deleted ID: 468\\nremained ID: 543\\nremained ID: 526\\nremained ID: 926\\n***** deleted ID: 60\\nremained ID: 28\\n***** deleted ID: 387\\nremained ID: 139\\n***** deleted ID: 312\\n***** deleted ID: 92\\nremained ID: 52\\n***** deleted ID: 764\\n***** deleted ID: 715\\n***** deleted ID: 999\\nremained ID: 20\\nremained ID: 1121\\nremained ID: 918\\nremained ID: 909\\nremained ID: 822\\nremained ID: 19\\nremained ID: 982\\n***** deleted ID: 823\\n***** deleted ID: 815\\n***** deleted ID: 807\\n***** deleted ID: 799\\n***** deleted ID: 790\\n***** deleted ID: 782\\n***** deleted ID: 775\\n***** deleted ID: 766\\n***** deleted ID: 758\\n***** deleted ID: 751\\n***** deleted ID: 742\\n***** deleted ID: 734\\n***** deleted ID: 10702\\nremained ID: 632\\nremained ID: 10704\\nremained ID: 10703\\nremained ID: 726\\n***** deleted ID: 504\\n***** deleted ID: 495\\n***** deleted ID: 486\\n***** deleted ID: 479\\n***** deleted ID: 471\\nremained ID: 463\\n***** deleted ID: 454\\n***** deleted ID: 446\\n***** deleted ID: 438\\n***** deleted ID: 431\\nremained ID: 423\\n***** deleted ID: 415\\n***** deleted ID: 407\\n***** deleted ID: 399\\n***** deleted ID: 391\\nremained ID: 382\\nremained ID: 375\\nremained ID: 1080\\nremained ID: 1089\\n***** deleted ID: 1142\\n***** deleted ID: 1141\\n***** deleted ID: 1140\\n***** deleted ID: 517\\nremained ID: 566\\n***** deleted ID: 424\\n***** deleted ID: 416\\n***** deleted ID: 408\\n***** deleted ID: 400\\nremained ID: 788\\n***** deleted ID: 256\\n***** deleted ID: 248\\n***** deleted ID: 240\\n***** deleted ID: 383\\n***** deleted ID: 592\\nremained ID: 663\\n***** deleted ID: 232\\n***** deleted ID: 224\\n***** deleted ID: 216\\n***** deleted ID: 376\\n***** deleted ID: 584\\nremained ID: 655\\nremained ID: 647\\n***** deleted ID: 208\\n***** deleted ID: 200\\n***** deleted ID: 192\\nremained ID: 639\\nremained ID: 631\\nremained ID: 1139\\nremained ID: 268\\nremained ID: 260\\n***** deleted ID: 392\\nremained ID: 619\\n***** deleted ID: 291\\n***** deleted ID: 284\\n***** deleted ID: 276\\n***** deleted ID: 152\\nremained ID: 961\\n***** deleted ID: 267\\n***** deleted ID: 646\\n***** deleted ID: 360\\n***** deleted ID: 535\\n***** deleted ID: 496\\nremained ID: 814\\n***** deleted ID: 1082\\n***** deleted ID: 1075\\n***** deleted ID: 1067\\n***** deleted ID: 306\\nremained ID: 605\\n***** deleted ID: 1059\\n***** deleted ID: 1050\\n***** deleted ID: 1042\\n***** deleted ID: 1034\\n***** deleted ID: 297\\nremained ID: 597\\nremained ID: 589\\n***** deleted ID: 168\\n***** deleted ID: 160\\n***** deleted ID: 199\\n***** deleted ID: 191\\n***** deleted ID: 183\\n***** deleted ID: 175\\n***** deleted ID: 167\\nremained ID: 159\\nremained ID: 204\\nremained ID: 196\\nremained ID: 188\\nremained ID: 151\\n***** deleted ID: 244\\n***** deleted ID: 236\\n***** deleted ID: 228\\n***** deleted ID: 220\\n***** deleted ID: 212\\nremained ID: 507\\nremained ID: 698\\nremained ID: 1045\\nremained ID: 977\\nremained ID: 988\\nremained ID: 427\\nremained ID: 836\\nremained ID: 895\\nremained ID: 368\\nremained ID: 335\\nremained ID: 692\\nremained ID: 888\\nremained ID: 540\\nremained ID: 922\\nremained ID: 786\\nremained ID: 729\\nremained ID: 289\\nremained ID: 234\\nremained ID: 1127\\nremained ID: 97\\nremained ID: 541\\nremained ID: 312782624\\nremained ID: 312782620\\nremained ID: 312782616'},\n",
       "     {'output_type': 'stream',\n",
       "      'name': 'stdout',\n",
       "      'text': '\\nremained ID: 312782612\\nremained ID: 312782608\\nremained ID: 312782604\\nremained ID: 417\\nremained ID: 312782570\\nremained ID: 312782566\\nremained ID: 312782562\\nremained ID: 312782558\\nremained ID: 312782554\\nremained ID: 312782550\\nremained ID: 312782546\\n***** deleted ID: 340\\n***** deleted ID: 308\\n***** deleted ID: 683\\n***** deleted ID: 635\\n***** deleted ID: 241\\n***** deleted ID: 532\\nremained ID: 22\\nremained ID: 622\\nremained ID: 590\\nremained ID: 687\\nremained ID: 430\\n***** deleted ID: 606\\nremained ID: 542\\nremained ID: 886\\nremained ID: 330\\nremained ID: 274\\nremained ID: 610\\n***** deleted ID: 545\\nremained ID: 434\\nremained ID: 442\\nremained ID: 879\\n***** deleted ID: 480149338\\n***** deleted ID: 480149334\\n***** deleted ID: 480149330\\n***** deleted ID: 480149326\\n***** deleted ID: 480149322\\n***** deleted ID: 480149318\\n***** deleted ID: 480149314\\n***** deleted ID: 480149310\\n***** deleted ID: 480149306\\n***** deleted ID: 480149302\\n***** deleted ID: 480149298\\n***** deleted ID: 480149294\\n***** deleted ID: 480149290\\n***** deleted ID: 480149286\\n***** deleted ID: 480149282\\n***** deleted ID: 480149278\\n***** deleted ID: 480149274\\n***** deleted ID: 480149270\\n***** deleted ID: 480149266\\n***** deleted ID: 480149262\\n***** deleted ID: 480149258\\nremained ID: 279\\nremained ID: 906\\nremained ID: 774\\nremained ID: 965\\nremained ID: 671\\nremained ID: 894\\nremained ID: 254\\nremained ID: 699\\nremained ID: 675\\nremained ID: 800\\nremained ID: 694\\nremained ID: 704\\nremained ID: 119\\nremained ID: 355\\nremained ID: 314\\nremained ID: 344\\nremained ID: 163\\nremained ID: 120\\nremained ID: 111\\nremained ID: 831\\nremained ID: 783\\nremained ID: 1101\\nremained ID: 328\\nremained ID: 996\\nremained ID: 104\\nremained ID: 95\\nremained ID: 680\\nremained ID: 608\\nremained ID: 1125\\nremained ID: 288\\nremained ID: 969\\nremained ID: 746\\n***** deleted ID: 738\\nremained ID: 527696977\\nremained ID: 910\\nremained ID: 620\\nremained ID: 582\\n***** deleted ID: 524\\nremained ID: 484\\nremained ID: 731\\nremained ID: 488\\nremained ID: 440\\nremained ID: 630\\nremained ID: 412\\nremained ID: 448\\nremained ID: 723\\n***** deleted ID: 516\\n***** deleted ID: 476\\n***** deleted ID: 352\\n***** deleted ID: 492\\n***** deleted ID: 264\\nremained ID: 714\\nremained ID: 1081\\nremained ID: 1054\\nremained ID: 827\\nremained ID: 556\\n***** deleted ID: 747\\nremained ID: 707\\nremained ID: 44\\nremained ID: 132\\nremained ID: 84\\nremained ID: 363\\nremained ID: 304\\n***** deleted ID: 195\\nremained ID: 171\\nremained ID: 972\\nremained ID: 819\\nremained ID: 810\\nremained ID: 772\\nremained ID: 296\\nremained ID: 588\\nremained ID: 48\\nremained ID: 927\\nremained ID: 919\\nremained ID: 1015\\nremained ID: 211\\nremained ID: 935\\nremained ID: 39\\n***** deleted ID: 227\\n***** deleted ID: 179\\n***** deleted ID: 739\\n***** deleted ID: 1053\\n***** deleted ID: 572\\nremained ID: 31\\nremained ID: 312782652\\nremained ID: 312782648\\nremained ID: 312782644\\nremained ID: 312782640\\nremained ID: 312782636\\nremained ID: 312782632\\nremained ID: 312782628\\nremained ID: 312782598\\nremained ID: 312782594\\nremained ID: 312782590\\nremained ID: 312782586\\nremained ID: 312782582\\nremained ID: 312782578\\nremained ID: 312782574\\nremained ID: 469\\nremained ID: 257\\nremained ID: 565\\nremained ID: 501\\nremained ID: 41\\nremained ID: 805\\nremained ID: 533\\nremained ID: 393\\nremained ID: 377\\nremained ID: 902\\nremained ID: 869\\nremained ID: 269\\nremained ID: 750\\nremained ID: 425\\nremained ID: 305\\nremained ID: 33\\nremained ID: 778\\nremained ID: 721\\nremained ID: 821\\nremained ID: 593\\nremained ID: 385\\nremained ID: 121\\nremained ID: 74\\nremained ID: 613\\nremained ID: 573\\nremained ID: 973\\nremained ID: 421\\nremained ID: 409\\nremained ID: 441\\nremained ID: 1046\\nremained ID: 433\\nremained ID: 401\\nremained ID: 1066\\nremained ID: 281\\nremained ID: 394\\nremained ID: 649\\nremained ID: 601\\nremained ID: 233\\nremained ID: 1114\\nremained ID: 905\\nremained ID: 1074\\nremained ID: 402\\n***** deleted ID: 497\\n***** deleted ID: 457\\n***** deleted ID: 937\\n***** deleted ID: 913\\n***** deleted ID: 561\\n***** deleted ID: 801\\nremained ID: 669\\nremained ID: 598\\nremained ID: 520\\nremained ID: 1023\\nremained ID: 990\\nremained ID: 755\\nremained ID: 959\\nremained ID: 1018\\nremained ID: 456\\nremained ID: 249\\nremained ID: 791\\nremained ID: 759\\nremained ID: 643\\nremained ID: 696\\nremained ID: 1027\\nremained ID: 1005\\nremained ID: 954\\nremained ID: 847\\nremained ID: 816\\nremained ID: 251\\nremained ID: 735\\nremained ID: 1002\\n***** deleted ID: 480149254\\n***** deleted ID: 480149250\\n***** deleted ID: 480149246\\n***** deleted ID: 480149242\\n***** deleted ID: 480149238\\n***** deleted ID: 480149234\\n***** deleted ID: 480149230\\nremained ID: 243\\nremained ID: 156\\nremained ID: 252\\nremained ID: 678\\nremained ID: 600\\nremained ID: 527\\nremained ID: 1011\\nremained ID: 247\\nremained ID: 849\\nremained ID: 857\\nremained ID: 1058\\nremained ID: 1010\\nremained ID: 1106\\nremained ID: 897\\nremained ID: 677\\nremained ID: 662\\nremained ID: 638\\nremained ID: 187\\nremained ID: 148\\nremained ID: 180\\nremained ID: 36\\nremained ID: 1057\\nremained ID: 893\\nremained ID: 862\\nremained ID: 1090\\nremained ID: 1035\\nremained ID: 806\\nremained ID: 873\\nremained ID: 378\\nremained ID: 182305713\\nremained ID: 182305709\\nremained ID: 182305705\\nremained ID: 182305701\\nremained ID: 182305697\\nremained ID: 182305693\\nremained ID: 182305689\\nremained ID: 461\\nremained ID: 9\\nremained ID: 1111\\nremained ID: 1086\\nremained ID: 670\\nremained ID: 1006\\nremained ID: 361\\nremained ID: 1026\\nremained ID: 945\\nremained ID: 625\\nremained ID: 577\\nremained ID: 854\\nremained ID: 450\\nremained ID: 369\\nremained ID: 2\\nremained ID: 1102\\nremained ID: 974\\nremained ID: 950\\nremained ID: 657\\nremained ID: 878\\nremained ID: 345\\nremained ID: 510\\nremained ID: 478\\nremained ID: 1128\\nremained ID: 1094\\nremained ID: 113\\nremained ID: 1030\\nremained ID: 337\\n***** deleted ID: 480149226\\n***** deleted ID: 480149222\\n***** deleted ID: 480149218\\n***** deleted ID: 480149214\\n***** deleted ID: 480149210\\n***** deleted ID: 480149206\\n***** deleted ID: 480149202\\nremained ID: 1062\\nremained ID: 1038\\nremained ID: 1070\\nremained ID: 1047\\nremained ID: 201\\nremained ID: 981\\nremained ID: 329\\nremained ID: 929\\nremained ID: 889\\nremained ID: 702\\nremained ID: 654\\nremained ID: 838\\nremained ID: 558\\nremained ID: 353\\n***** deleted ID: 719\\n***** deleted ID: 686\\n***** deleted ID: 921\\n***** deleted ID: 865\\n***** deleted ID: 346\\n***** deleted ID: 793\\nremained ID: 322\\n***** deleted ID: 12998\\n***** deleted ID: 12997\\n***** deleted ID: 12996\\n***** deleted ID: 12995\\n***** deleted ID: 12994\\n***** deleted ID: 12993\\nremained ID: 453\\nremained ID: 1085\\nremained ID: 1021\\nremained ID: 767\\nremained ID: 962\\nremained ID: 656\\nremained ID: 993\\nremained ID: 882\\nremained ID: 844\\nremained ID: 648\\nremained ID: 943\\nremained ID: 320\\nremained ID: 985\\n***** deleted ID: 947\\n***** deleted ID: 644\\n***** deleted ID: 299\\n***** deleted ID: 219\\n***** deleted ID: 107\\nremained ID: 500\\nremained ID: 526322264\\nremained ID: 526157196\\nremained ID: 526157192\\nremained ID: 667\\nremained ID: 68\\nremained ID: 184\\nremained ID: 315\\nremained ID: 695\\nremained ID: 688\\nremained ID: 567\\nremained ID: 8\\nremained ID: 997\\n'}],\n",
       "    'source': \"leaf_count = 0\\njsonpath_expr = parse('$..id')\\ndf_VC_pruned = copy.deepcopy(df_VC)\\ntemp_list = []\\ntemp_list = [Delete_leaf_node_WO_VC(match.value, match.full_path)\\\\\\n            for match in reversed(jsonpath_expr.find(df_VC_pruned['msg'][0]))]\\nDeletedLeafs = np.array(temp_list)\"},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Save a pruned annotation ontology file with voxel counts '},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 6,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': \"with open(os.path.join(dir_data, fn_output_AO), mode='w') as fw:\\n    json.dump(df_VC_pruned, fw, indent=4)\"},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Check data'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 7,\n",
       "    'metadata': {},\n",
       "    'outputs': [{'output_type': 'stream',\n",
       "      'name': 'stdout',\n",
       "      'text': '# all IDs in AO: 1327\\n# remained IDs: 837\\n# deleted IDs: 490\\n# leaf count: 1130\\n'}],\n",
       "    'source': \"print('# all IDs in AO: '+str(len(DeletedLeafs))) # 1327\\nprint('# remained IDs: '+str(sum(DeletedLeafs>0))) # 837\\nprint('# deleted IDs: '+str(sum(DeletedLeafs<0))) # 490\\nprint('# leaf count: ' +str(leaf_count))\"}],\n",
       "  'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "    'language': 'python',\n",
       "    'name': 'python3'},\n",
       "   'language_info': {'name': 'python',\n",
       "    'version': '3.7.1',\n",
       "    'mimetype': 'text/x-python',\n",
       "    'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "    'pygments_lexer': 'ipython3',\n",
       "    'nbconvert_exporter': 'python',\n",
       "    'file_extension': '.py'},\n",
       "   'toc': {'base_numbering': 1,\n",
       "    'nav_menu': {},\n",
       "    'number_sections': True,\n",
       "    'sideBar': True,\n",
       "    'skip_h1_title': False,\n",
       "    'title_cell': 'Table of Contents',\n",
       "    'title_sidebar': 'Contents',\n",
       "    'toc_cell': False,\n",
       "    'toc_position': {'height': 'calc(100% - 180px)',\n",
       "     'left': '10px',\n",
       "     'top': '150px',\n",
       "     'width': '185.4px'},\n",
       "    'toc_section_display': True,\n",
       "    'toc_window_display': True}},\n",
       "  'nbformat': 4,\n",
       "  'nbformat_minor': 2},\n",
       " {'metadata': {'path': './notebooks'}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(dir_notebooks,'Prune_leaf_ROI_wo_VC_in_AO.ipynb')) as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "orig_parameters = extract_parameters(nb)\n",
    "params = parameter_values(orig_parameters, dir_data=path_datafolder)\n",
    "new_nb = replace_definitions(nb, params)\n",
    "\n",
    "ep = ExecutePreprocessor(timeout=600, kernel_name='python3')\n",
    "ep.preprocess(new_nb, {'metadata':{'path':'./notebooks'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide_internal_ROI_with_VC_in_AO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cells': [{'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Divide internal ROIs that have voxel counts\\nThis notebook divides ROIs in an anatomical ontology (AO) text-file if these are internal nodes with voxel counts (VC) > 0 in an annotation volume (AV). A divided ROI has a name and acronym suffixed with \"peripheral\" such as \"original name_peripheral\" and \"original acronym_peri\". ID of divided ROI is newly assigned (>= 10^9).\\n\\n- input\\n    - 1_VC_pruned.json\\n- outputs\\n    - 1_VC_pruned_divided.json\\n    - dividedIDs.csv'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Set variables'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 1,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': \"dir_data = 'D:\\\\\\\\ES\\\\\\\\Projects\\\\\\\\FlexibleAtlas\\\\\\\\github\\\\\\\\data'\\nfn_input_AO = '1_VC_pruned.json'\\nfn_output_AO = '1_VC_pruned_divided.json'\\nfn_output_ID = 'dividedIDs.csv'\"},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 2,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'import os\\nimport nrrd\\nimport numpy as np\\nimport pandas as pd\\nimport json\\nimport copy\\nfrom collections import OrderedDict\\nfrom jsonpath_rw import jsonpath, parse'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Load data'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 3,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'with open(os.path.join(dir_data, fn_input_AO)) as f:\\n    df_AO_VC_pruned = json.load(f, object_pairs_hook=OrderedDict)'},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Get ID-acronym to prepare candidate ID for divided ROI'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 4,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'jsonpath_expr = parse(\\'$..id\\')\\n\\nIDacronym_list = [[match.value, \\\\\\n                  eval(\"df_AO_VC_pruned[\\'msg\\'][0]\" +str(match.full_path).\\\\\\n                      replace(\\'.\\',\\'\\').replace(\\'children\\',\"[\\'children\\']\").\\\\\\n                      replace(\\'id\\',\"\") + \"[\\'acronym\\']\"),\\\\\\n                  eval(\"df_AO_VC_pruned[\\'msg\\'][0]\" +str(match.full_path).\\\\\\n                      replace(\\'.\\',\\'\\').replace(\\'children\\',\"[\\'children\\']\").\\\\\\n                      replace(\\'id\\',\"\") + \"[\\'name\\']\")]\\n                 for match in jsonpath_expr.find(df_AO_VC_pruned[\\'msg\\'][0])]\\nIDacronym = pd.DataFrame(IDacronym_list, columns=[\\'ID\\', \\'acronym\\', \\'name\\'])'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 5,\n",
       "    'metadata': {},\n",
       "    'outputs': [{'output_type': 'stream',\n",
       "      'name': 'stdout',\n",
       "      'text': '614454277\\n'}],\n",
       "    'source': \"maxID = IDacronym['ID'].max() # 614454277 in the original annotation ontology file\\nprint(maxID)\"},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 6,\n",
       "    'metadata': {},\n",
       "    'outputs': [{'output_type': 'execute_result',\n",
       "      'metadata': {},\n",
       "      'data': {'text/plain': '999999999'},\n",
       "      'execution_count': 6}],\n",
       "    'source': 'if maxID < 10**9:\\n    CandidateID = 10**9 -1 # -1 to start from 10**9, not \"10**9 + 1\"\\nelse:\\n    CandidateID = maxID + 1\\nCandidateID'},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Divide internal nodes with voxel counts > 0'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 7,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'def Divide_internal_ROI_with_VC_in_AO(match_id, match_fullpath):\\n    if match_id == 0: return # Children is not defined for a root node.\\n    # id_offset = 10**9 # ID of divided node = original ID + this offset\\n    global df_AO_VC_pruned_leafed\\n    global CandidateID\\n    bool1 = eval((\"df_AO_VC_pruned_leafed\"+\\\\\\n                str(match_fullpath).replace(\\'.\\',\\'\\')\\\\\\n                .replace(\\'msg\\', \"[\\'msg\\']\")\\\\\\n                .replace(\\'children\\', \"[\\'children\\']\")\\\\\\n                .replace(\\'id\\',\\'\\')+\"[\\'children\\'] != []\")) # true if non-leaf node\\n    bool2 = eval((\"df_AO_VC_pruned_leafed\"+\\\\\\n                str(match_fullpath).replace(\\'.\\',\\'\\')\\\\\\n                .replace(\\'msg\\', \"[\\'msg\\']\")\\\\\\n                .replace(\\'children\\', \"[\\'children\\']\")\\\\\\n                .replace(\\'id\\',\\'\\')+\"[\\'voxel_count\\'] is not None\")) # true if voxel_count > 0\\n    if bool1 and bool2:\\n        CandidateID += 1 \\n        source_acronym =  eval(\"df_AO_VC_pruned_leafed\"+\\\\\\n                str(match_fullpath).replace(\\'.\\',\\'\\')\\\\\\n                .replace(\\'msg\\', \"[\\'msg\\']\").replace(\\'children\\', \"[\\'children\\']\")\\\\\\n                .replace(\\'id\\', \"[\\'acronym\\']\")) # acronym for matched ID\\n        source_name =  eval(\"df_AO_VC_pruned_leafed\"+\\\\\\n                str(match_fullpath).replace(\\'.\\',\\'\\')\\\\\\n                .replace(\\'msg\\', \"[\\'msg\\']\").replace(\\'children\\', \"[\\'children\\']\")\\\\\\n                .replace(\\'id\\', \"[\\'name\\']\")) # name for matched ID\\n        source_original_content =  eval(\"copy.deepcopy(df_AO_VC_pruned_leafed\"+\\\\\\n                str(match_fullpath).replace(\\'.\\',\\'\\')\\\\\\n                .replace(\\'msg\\', \"[\\'msg\\']\")\\\\\\n                .replace(\\'children\\', \"[\\'children\\']\").replace(\\'id\\', \"\")+\")\") # OrderedDict\\n        source_child_index = int(str(match_fullpath)\\\\\\n            [str(match_fullpath).rfind(\"[\")+1:str(match_fullpath).rfind(\"]\")])\\n        source_parent_path = \"df_AO_VC_pruned_leafed\"+str(match_fullpath)\\\\\\n            [0:str(match_fullpath).rfind(\"[\")-1].replace(\\'.\\',\\'\\')\\\\\\n            .replace(\\'msg\\', \"[\\'msg\\']\")\\\\\\n            .replace(\\'children\\', \"[\\'children\\']\") # ./children[x]./children\\n        source_original_voxelcount = eval((\"df_AO_VC_pruned_leafed\"+\\\\\\n                str(match_fullpath).replace(\\'.\\',\\'\\').replace(\\'msg\\', \"[\\'msg\\']\")\\\\\\n                .replace(\\'children\\', \"[\\'children\\']\").replace(\\'id\\',\\'\\')+\"[\\'voxel_count\\']\"))\\n        exec(source_parent_path + \"[\" +str(source_child_index) +\"][\\'voxel_count\\'] = None\")\\n        exec(source_parent_path+\\\\\\n                \".insert(\" + str(source_child_index + 1) + \", source_original_content)\") \\n        exec(source_parent_path + \"[\" + str(source_child_index + 1) + \"][\\'acronym\\'] = \\'\"\\\\\\n             + source_acronym + \"_peri\\'\")\\n        exec(source_parent_path + \"[\" + str(source_child_index + 1) + \"][\\'name\\'] = \\'\"\\\\\\n             + source_name + \"_peripheral\\'\")\\n        exec(source_parent_path + \"[\" + str(source_child_index + 1) + \"][\\'children\\'] = None\")\\n        exec(source_parent_path + \"[\" + str(source_child_index + 1) + \"][\\'id\\'] = \"\\\\\\n             + str(CandidateID))\\n        exec(source_parent_path + \"[\" + str(source_child_index + 1) +\\\\\\n             \"][\\'voxel_count\\'] = \" + str(source_original_voxelcount))\\n        return [match_id, source_acronym, source_name, source_original_voxelcount, CandidateID] # Leafed ID and its acronym'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 8,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': \"jsonpath_expr = parse('$..id')\\ndf_AO_VC_pruned_leafed = copy.deepcopy(df_AO_VC_pruned)\\nID_divided = []\\nID_divided = [Divide_internal_ROI_with_VC_in_AO(match.value, match.full_path)\\\\\\n            for match in reversed(jsonpath_expr.find(df_AO_VC_pruned_leafed))]\"},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Get internal ROIs with voxel size > 0'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 9,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': \"ID_divided_wo_None = []\\nID_divided_wo_None = [x for x in ID_divided if x is not None]\\ndf_dividedIDs = pd.DataFrame(ID_divided_wo_None, columns=['divided_ID', 'acronym', 'name', 'voxel_count', 'new_ID'])\"},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Save AO and csv files'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '## AO json-file'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 10,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': \"with open(os.path.join(dir_data, fn_output_AO), mode='w') as fw:\\n    json.dump(df_AO_VC_pruned_leafed, fw, indent=4)\"},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '## csv file with divided IDs'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 11,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'df_dividedIDs.to_csv(os.path.join(dir_data, fn_output_ID), index=False)'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Check data'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 12,\n",
       "    'metadata': {},\n",
       "    'outputs': [{'output_type': 'execute_result',\n",
       "      'metadata': {},\n",
       "      'data': {'text/plain': '   divided_ID acronym                 name  voxel_count      new_ID\\n0         145      V4     fourth ventricle          523  1000000000\\n1          81      VL    lateral ventricle         2138  1000000001\\n2         301      st     stria terminalis          292  1000000002\\n3         863    rust    rubrospinal tract          564  1000000003\\n4         784     cst  corticospinal tract           92  1000000004',\n",
       "       'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>divided_ID</th>\\n      <th>acronym</th>\\n      <th>name</th>\\n      <th>voxel_count</th>\\n      <th>new_ID</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>145</td>\\n      <td>V4</td>\\n      <td>fourth ventricle</td>\\n      <td>523</td>\\n      <td>1000000000</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>81</td>\\n      <td>VL</td>\\n      <td>lateral ventricle</td>\\n      <td>2138</td>\\n      <td>1000000001</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>301</td>\\n      <td>st</td>\\n      <td>stria terminalis</td>\\n      <td>292</td>\\n      <td>1000000002</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>863</td>\\n      <td>rust</td>\\n      <td>rubrospinal tract</td>\\n      <td>564</td>\\n      <td>1000000003</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>784</td>\\n      <td>cst</td>\\n      <td>corticospinal tract</td>\\n      <td>92</td>\\n      <td>1000000004</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'},\n",
       "      'execution_count': 12}],\n",
       "    'source': 'df_dividedIDs.head() # .shape (29, 3) There were 29 inner nodes with voxel counts > 0.'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 13,\n",
       "    'metadata': {},\n",
       "    'outputs': [{'output_type': 'execute_result',\n",
       "      'metadata': {},\n",
       "      'data': {'text/plain': '614454277'},\n",
       "      'execution_count': 13}],\n",
       "    'source': \"IDacronym['ID'].max()\"},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 14,\n",
       "    'metadata': {},\n",
       "    'outputs': [{'output_type': 'stream',\n",
       "      'name': 'stdout',\n",
       "      'text': '614454277\\n8.788489571542424\\n9.632959861247398\\n4.294375337334885\\n'}],\n",
       "    'source': \"import math\\nprint(IDacronym['ID'].max())\\nprint(math.log10(IDacronym['ID'].max()))\\nprint(math.log10(2**32))\\nprint(10**0.6329)\"}],\n",
       "  'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "    'language': 'python',\n",
       "    'name': 'python3'},\n",
       "   'language_info': {'name': 'python',\n",
       "    'version': '3.7.1',\n",
       "    'mimetype': 'text/x-python',\n",
       "    'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "    'pygments_lexer': 'ipython3',\n",
       "    'nbconvert_exporter': 'python',\n",
       "    'file_extension': '.py'},\n",
       "   'toc': {'base_numbering': 1,\n",
       "    'nav_menu': {},\n",
       "    'number_sections': True,\n",
       "    'sideBar': True,\n",
       "    'skip_h1_title': False,\n",
       "    'title_cell': 'Table of Contents',\n",
       "    'title_sidebar': 'Contents',\n",
       "    'toc_cell': False,\n",
       "    'toc_position': {'height': '946px',\n",
       "     'left': '0px',\n",
       "     'top': '92px',\n",
       "     'width': '336.6px'},\n",
       "    'toc_section_display': True,\n",
       "    'toc_window_display': True}},\n",
       "  'nbformat': 4,\n",
       "  'nbformat_minor': 2},\n",
       " {'metadata': {'path': './notebooks'}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(dir_notebooks,'Divide_internal_ROI_with_VC_in_AO.ipynb'),\\\n",
    "                      encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "orig_parameters = extract_parameters(nb)\n",
    "params = parameter_values(orig_parameters, dir_data=path_datafolder)\n",
    "new_nb = replace_definitions(nb, params)\n",
    "\n",
    "ep = ExecutePreprocessor(timeout=600, kernel_name='python3')\n",
    "ep.preprocess(new_nb, {'metadata':{'path':'./notebooks'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update_ID_in_AV_to_reflect_divided_AO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cells': [{'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Update IDs in annotation volume according to divided anatomical ontology\\nThis notebook updates IDs of divided nodes from original ID.\\n\\n- inputs\\n    - annotation_100.nrrd\\n    - dividedIDs.csv\\n- outputs\\n    - annotation_100_divided.nrrd'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Set variables'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 1,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': \"dir_data = 'D:\\\\\\\\ES\\\\\\\\Projects\\\\\\\\FlexibleAtlas\\\\\\\\github\\\\\\\\data'\\nfn_input_AV = 'annotation_100.nrrd'\\nfn_input_ID = 'dividedIDs.csv'\\nfn_output_AV = 'AVbase.nrrd'\"},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 2,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'import os\\nimport nrrd\\nimport numpy as np\\nimport pandas as pd\\nimport copy'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Load data'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 3,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'AV, Header = nrrd.read(os.path.join(dir_data, fn_input_AV))\\ndf_dividedIDs = pd.read_csv(os.path.join(dir_data, fn_input_ID))'},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Modify IDs for divided ROIs'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 4,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'AV_modified = copy.deepcopy(AV)\\nfor idx, originalID in enumerate(df_dividedIDs.divided_ID):\\n    AV_modified[AV_modified == originalID] = df_dividedIDs.iloc[idx,:].new_ID'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Save divided AV'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 5,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'nrrd.write(os.path.join(dir_data, fn_output_AV), AV_modified, Header)'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Check data'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 6,\n",
       "    'metadata': {},\n",
       "    'outputs': [{'output_type': 'execute_result',\n",
       "      'metadata': {},\n",
       "      'data': {'text/plain': '   divided_ID acronym                 name  voxel_count      new_ID\\n0         145      V4     fourth ventricle          523  1000000000\\n1          81      VL    lateral ventricle         2138  1000000001\\n2         301      st     stria terminalis          292  1000000002\\n3         863    rust    rubrospinal tract          564  1000000003\\n4         784     cst  corticospinal tract           92  1000000004',\n",
       "       'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>divided_ID</th>\\n      <th>acronym</th>\\n      <th>name</th>\\n      <th>voxel_count</th>\\n      <th>new_ID</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>145</td>\\n      <td>V4</td>\\n      <td>fourth ventricle</td>\\n      <td>523</td>\\n      <td>1000000000</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>81</td>\\n      <td>VL</td>\\n      <td>lateral ventricle</td>\\n      <td>2138</td>\\n      <td>1000000001</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>301</td>\\n      <td>st</td>\\n      <td>stria terminalis</td>\\n      <td>292</td>\\n      <td>1000000002</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>863</td>\\n      <td>rust</td>\\n      <td>rubrospinal tract</td>\\n      <td>564</td>\\n      <td>1000000003</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>784</td>\\n      <td>cst</td>\\n      <td>corticospinal tract</td>\\n      <td>92</td>\\n      <td>1000000004</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'},\n",
       "      'execution_count': 6}],\n",
       "    'source': 'df_dividedIDs.head() # This is the same as dividedIDs.csv'}],\n",
       "  'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "    'language': 'python',\n",
       "    'name': 'python3'},\n",
       "   'language_info': {'name': 'python',\n",
       "    'version': '3.7.1',\n",
       "    'mimetype': 'text/x-python',\n",
       "    'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "    'pygments_lexer': 'ipython3',\n",
       "    'nbconvert_exporter': 'python',\n",
       "    'file_extension': '.py'},\n",
       "   'toc': {'base_numbering': 1,\n",
       "    'nav_menu': {},\n",
       "    'number_sections': True,\n",
       "    'sideBar': False,\n",
       "    'skip_h1_title': False,\n",
       "    'title_cell': 'Table of Contents',\n",
       "    'title_sidebar': 'Contents',\n",
       "    'toc_cell': False,\n",
       "    'toc_position': {'height': '946px',\n",
       "     'left': '36px',\n",
       "     'top': '92px',\n",
       "     'width': '354.2px'},\n",
       "    'toc_section_display': True,\n",
       "    'toc_window_display': True}},\n",
       "  'nbformat': 4,\n",
       "  'nbformat_minor': 2},\n",
       " {'metadata': {'path': './notebooks'}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(dir_notebooks,'Update_ID_in_AV_to_reflect_divided_AO.ipynb'),\\\n",
    "                      encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "orig_parameters = extract_parameters(nb)\n",
    "params = parameter_values(orig_parameters,\\\n",
    "                          dir_data=path_datafolder,\\\n",
    "                          fn_input_AV = 'annotation_100.nrrd',\\\n",
    "                          fn_input_ID = 'dividedIDs.csv',\\\n",
    "                          fn_output_AV = 'AVbase.nrrd')\n",
    "new_nb = replace_definitions(nb, params)\n",
    "\n",
    "ep = ExecutePreprocessor(timeout=600, kernel_name='python3')\n",
    "ep.preprocess(new_nb, {'metadata':{'path':'./notebooks'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add_VC_to_AO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cells': [{'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': \"# Add voxel-counts in annotation volume to an anatomical ontology file\\nUsing the original annotation volume (AV) and anatomical ontology (AO) file of the mouse brain by Allen Institute for Brain Science (AIBS), this notebook checks voxel-counts (VC) for each brain structure in AV, and adds them to AO file.\\n\\nPrepare directories 'data', 'gene_data', 'fiber_data' and 'figs' at a working directory. Put the original files by AIBS, '1.json' and 'annotation_100.nrrd', at 'data' folder. Output files are saved in the above folders.\\n\\n- input files\\n    - annotation_100.nrrd: The original AV by AIBS\\n    - 1.json: The original AO file by AIBS\\n- output file\\n    - 1_VC.json: AO file with voxel counts for each brain structure\"},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Set variables'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 1,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': \"dir_data = 'D:\\\\\\\\ES\\\\\\\\Projects\\\\\\\\FlexibleAtlas\\\\\\\\github\\\\\\\\data'\\n_case = 'VC_to_AO_original'\\nfn_input_AV = 'AVbase.nrrd'\\nfn_input_AO = '1_VC_pruned_divided.json'\\nfn_output_AO = 'AObase.json'\\nfn_input_AV = 'AVbase.nrrd'\\nfn_input_AO = '1_VC_pruned_divided.json'\\nfn_output_AO = 'AObase.json'\\nfn_input_AV = 'AVbase.nrrd'\\nfn_input_AO = '1_VC_pruned_divided.json'\\nfn_output_AO = 'AObase.json'\\nfn_input_AV = 'AVbase.nrrd'\\nfn_input_AO = '1_VC_pruned_divided.json'\\nfn_output_AO = 'AObase.json'\\nfn_input_AO = '1_VC_pruned_divided.json'\\nfn_input_AV = 'AVbase.nrrd'\\nfn_output_AO = 'AObase.json'\\nfn_input_AO = '1_VC_pruned_divided.json'\\nfn_input_AV = 'AVbase.nrrd'\\nfn_output_AO = 'AObase.json'\"},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 2,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': 'import os\\nimport nrrd\\nimport numpy as np\\nimport pandas as pd\\nimport json\\nimport copy\\nfrom collections import OrderedDict\\nfrom jsonpath_rw import jsonpath, parse'},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Load data'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 3,\n",
       "    'metadata': {'code_folding': []},\n",
       "    'outputs': [],\n",
       "    'source': \"AV, Header = nrrd.read(os.path.join(dir_data, fn_input_AV))\\n\\nID_unique = np.unique(AV)\\nIDvc_list = [[i, np.sum(AV == i)] for i in ID_unique]\\nIDvc = pd.DataFrame(IDvc_list, columns=['ID', 'voxel_count'])\\n\\nwith open(os.path.join(dir_data, fn_input_AO)) as f:\\n    df = json.load(f, object_pairs_hook=OrderedDict)\"},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Add voxel-counts to an annotation ontology file'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 4,\n",
       "    'metadata': {'code_folding': []},\n",
       "    'outputs': [],\n",
       "    'source': 'def Add_VC_to_AO(match_id, match_fullpath):\\n    global df_VC\\n    global IDvc\\n    if IDvc[\\'ID\\'].isin([match_id]).any():\\n        temp_voxelcount = IDvc.loc[IDvc[\\'ID\\'] == match_id, \\'voxel_count\\'].tolist()[0]\\n    else:\\n        temp_voxelcount = None\\n    exec(\"df_VC[\\'msg\\'][0]\"+\\\\\\n         str(match_fullpath).replace(\\'.\\',\\'\\')\\\\\\n          .replace(\\'children\\', \"[\\'children\\']\").replace(\\'id\\', \\'\\')+\\\\\\n         \"[\\'voxel_count\\'] = \"+ str(temp_voxelcount))\\n    return'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 5,\n",
       "    'metadata': {'code_folding': []},\n",
       "    'outputs': [],\n",
       "    'source': \"jsonpath_expr = parse('$..id')\\ndf_VC = copy.deepcopy(df)\\n[Add_VC_to_AO(match.value, match.full_path) \\\\\\n for match in jsonpath_expr.find(df_VC['msg'][0])];\"},\n",
       "   {'cell_type': 'markdown',\n",
       "    'metadata': {},\n",
       "    'source': '# Save AO with voxel counts'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 6,\n",
       "    'metadata': {},\n",
       "    'outputs': [],\n",
       "    'source': \"with open(os.path.join(dir_data, fn_output_AO), mode='w') as fw:\\n    json.dump(df_VC, fw, indent=4)\"},\n",
       "   {'cell_type': 'markdown', 'metadata': {}, 'source': '# Check data'},\n",
       "   {'cell_type': 'code',\n",
       "    'execution_count': 7,\n",
       "    'metadata': {},\n",
       "    'outputs': [{'output_type': 'stream',\n",
       "      'name': 'stdout',\n",
       "      'text': 'Data type of the annotation volume:  uint32\\n# unique ID:  670\\n# total voxel:  1203840\\n# brain voxel:  505359\\n# outside brain voxel:  698481\\n'},\n",
       "     {'output_type': 'execute_result',\n",
       "      'metadata': {},\n",
       "      'data': {'text/plain': '   ID  voxel_count\\n0   0       698481\\n1   1          107\\n2   2          119\\n3   6         2058\\n4   7         1108',\n",
       "       'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>ID</th>\\n      <th>voxel_count</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>0</td>\\n      <td>698481</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>1</td>\\n      <td>107</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2</td>\\n      <td>119</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>6</td>\\n      <td>2058</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>7</td>\\n      <td>1108</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'},\n",
       "      'execution_count': 7}],\n",
       "    'source': 'print(\"Data type of the annotation volume: \", AV.dtype) # uint32\\nprint(\"# unique ID: \", np.unique(AV).size) # There is 670 unique ID in AV\\nprint(\\'# total voxel: \\', np.size(AV)) # 1203840 voxels\\nprint(\\'# brain voxel: \\', np.sum(AV!=0)) # 505359\\nprint(\\'# outside brain voxel: \\', np.sum(AV==0)) # 698481\\nIDvc.head()'}],\n",
       "  'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "    'language': 'python',\n",
       "    'name': 'python3'},\n",
       "   'language_info': {'name': 'python',\n",
       "    'version': '3.7.1',\n",
       "    'mimetype': 'text/x-python',\n",
       "    'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "    'pygments_lexer': 'ipython3',\n",
       "    'nbconvert_exporter': 'python',\n",
       "    'file_extension': '.py'},\n",
       "   'toc': {'base_numbering': 1,\n",
       "    'nav_menu': {},\n",
       "    'number_sections': True,\n",
       "    'sideBar': True,\n",
       "    'skip_h1_title': False,\n",
       "    'title_cell': 'Table of Contents',\n",
       "    'title_sidebar': 'Contents',\n",
       "    'toc_cell': False,\n",
       "    'toc_position': {'height': 'calc(100% - 180px)',\n",
       "     'left': '10px',\n",
       "     'top': '150px',\n",
       "     'width': '384px'},\n",
       "    'toc_section_display': True,\n",
       "    'toc_window_display': True}},\n",
       "  'nbformat': 4,\n",
       "  'nbformat_minor': 2},\n",
       " {'metadata': {'path': './notebooks'}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(dir_notebooks,'Add_VC_to_AO.ipynb')) as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "    \n",
    "orig_parameters = extract_parameters(nb)\n",
    "params = parameter_values(orig_parameters,\\\n",
    "                          dir_data=path_datafolder,\\\n",
    "                          fn_input_AV='AVbase.nrrd',\\\n",
    "                          fn_input_AO='1_VC_pruned_divided.json',\\\n",
    "                          fn_output_AO='AObase.json')\n",
    "new_nb = replace_definitions(nb, params)\n",
    "\n",
    "ep = ExecutePreprocessor(timeout=600, kernel_name='python3')\n",
    "ep.preprocess(new_nb, {'metadata':{'path':'./notebooks'}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
